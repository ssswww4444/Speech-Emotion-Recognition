{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gated Convolutional Network for emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Conv1D, Input, MaxPooling1D\n",
    "from keras.layers import add, multiply\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/peiyun/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Set parameters for GCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len = 0\n",
    "max_len = 0\n",
    "count = 0\n",
    "for data_type in [\"train\", \"dev\", \"test\"]:\n",
    "    for config in [\"IS09_emotion\", \"IS10_paraling\"]:\n",
    "        d_path = os.path.join(data_path, \"output\", data_type, config)\n",
    "        for filename in os.listdir(d_path):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                with open(os.path.join(d_path, filename) , mode='r') as f:\n",
    "                    row_num = sum(1 for i in f)\n",
    "                    avg_len += row_num\n",
    "                    if row_num > max_len:\n",
    "                        max_len = row_num\n",
    "                    f.close()\n",
    "                count += 1\n",
    "avg_len /= count\n",
    "print avg_len\n",
    "print max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LEN = 3450 # 3449 (for each utterance)\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCH = 32\n",
    "KERNEL_SIZE = 2\n",
    "\n",
    "IS_DILATED = False\n",
    "DILATION_RATE = 1\n",
    "N_STACK = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Extracting the matrices and labels for all data types and store as numpy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for obtaining label according to the label.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ground_truth label number of the file\n",
    "def get_label(filename):\n",
    "    label_file = os.path.join(data_path, \"label.csv\")\n",
    "    \n",
    "    with open(label_file, mode = \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            name, session, label, dims, data_type, transcript = row\n",
    "            # print filename\n",
    "            if name == filename:\n",
    "                return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for extracting the corresponding normalised input and the ground truth output of a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_Xy(file_path):\n",
    "    \n",
    "    # Obtain dataframe for the csv file (one utterance/instance)\n",
    "    df = pd.read_csv(file_path, sep = \";\")\n",
    "    \n",
    "    # Get filename\n",
    "    filename = df[\"name\"][0][1:-1]  # [1:-1] for removing single quotation marks\n",
    "    \n",
    "    # Clean unnecessary columns\n",
    "    df = df.drop(columns = [\"name\", \"frameTime\"])\n",
    "    \n",
    "    # Normalise data\n",
    "    x = df.values                # dataframe to a numpy array\n",
    "    min_max_scalar = MinMaxScaler()  \n",
    "    x_scaled = min_max_scalar.fit_transform(x)     # scaling each feature (each column) to range: (0,1)\n",
    "    df = pd.DataFrame(x_scaled)  # numpy array back to dataframe\n",
    "    \n",
    "    \n",
    "    # Obtain matrix X and label y, X_dim: timesteps x features\n",
    "    x_data = sequence.pad_sequences(df.values.T, padding = \"post\", dtype = \"float64\", maxlen = MAX_LEN).T\n",
    "    \n",
    "    y_data = get_label(filename)\n",
    "    \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_Xy(\"/work/peiyun/data/output/train/IS09_emotion/Ses05M_script03_2_M040_IS09_emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS09 has 32 LLDs and IS10 has 76 LLDs\n",
    "def get_feature_num(config):\n",
    "    \n",
    "    # enter the folder for the config under train (can also use test/dev, same result)\n",
    "    path = os.path.join(data_path, \"output\", \"train\", config)\n",
    "    \n",
    "    # enter any config folder (nums of files are the same for all configs)\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            path = os.path.join(path, filename)\n",
    "            break\n",
    "    \n",
    "    # Obtain dataframe for the csv file (one utterance/instance)\n",
    "    df = pd.read_csv(path, sep = \";\")\n",
    "    \n",
    "    # Clean unnecessary columns\n",
    "    df = df.drop(columns = [\"name\", \"frameTime\"])\n",
    "\n",
    "    return len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the required matrix and label pairs for each data type and store as numpy files. \n",
    "<br>\n",
    "NOTE: np.load(filename.npy) to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_num(data_type):\n",
    "    \n",
    "    # enter the folder for the data_type\n",
    "    path = os.path.join(data_path, \"output\", data_type)\n",
    "    \n",
    "    # enter any config folder (nums of files are the same for all configs)\n",
    "    for config in os.listdir(path):\n",
    "        path = os.path.join(path, config)\n",
    "        break\n",
    "        \n",
    "    return len(os.listdir(path))/2   # only half are csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = os.path.join(data_path, \"temp\")\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"ang\": 0,\n",
    "              \"exc\": 1,\n",
    "              \"fru\": 2,\n",
    "              \"hap\": 3,\n",
    "              \"neu\": 4,\n",
    "              \"sad\": 5,\n",
    "              \"sur\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as numpy file\n",
    "np.save(os.path.join(data_path, \"label_dict.npy\"), label_dict)\n",
    "\n",
    "# # loading numpy file\n",
    "# label_dict = np.load(os.path.join(data_path, \"label_dict.npy\")).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_dir = \"numpy_var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(data_path, numpy_dir)):\n",
    "    os.makedirs(os.path.join(data_path, numpy_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in [\"IS09_emotion\", \"IS10_paraling\"]:\n",
    "    \n",
    "    for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "\n",
    "        # temp dat file for storing X\n",
    "        X_data_filename = os.path.join(temp_dir, data_type + \".dat\")\n",
    "        \n",
    "        # Obtain the number of features and number of files\n",
    "        feature_num = get_feature_num(config)\n",
    "        file_num = get_file_num(data_type)\n",
    "        \n",
    " \n",
    "        # initialise lists (using memory-map for accessing small segments of large files on disk)\n",
    "        X = np.memmap(X_data_filename, dtype='float64', mode='w+', shape=((file_num, MAX_LEN + KERNEL_SIZE - 1, feature_num)))\n",
    "        Y = []\n",
    "\n",
    "        # path for the directory of each data_type\n",
    "        path = os.path.join(data_path, \"output\", data_type, config)\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        # iterate through all files\n",
    "        for filename in os.listdir(path):\n",
    "\n",
    "            # only interested in csv files\n",
    "            if not filename.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            # extract data\n",
    "            x_data, y_data = extract_Xy(os.path.join(path, filename))\n",
    "            Y.append(label_dict[y_data])\n",
    "            \n",
    "            # Padding zeros to the beginning of the sequences \n",
    "            # with kernel_size - 1 elements to prevent kernels from seeing the future context\n",
    "            zeros = np.zeros((KERNEL_SIZE - 1, feature_num))\n",
    "            x_data = np.concatenate((zeros, x_data), axis = 0)\n",
    "            X[i] = x_data\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print \"num of file processed: \" + str(i)\n",
    "\n",
    "        # save as numpy file\n",
    "        np.save(os.path.join(data_path, numpy_dir, data_type + \"_\" + config + \"_X.npy\"), X)\n",
    "        np.save(os.path.join(data_path, numpy_dir, data_type + \"_\" + config + \"_Y.npy\"), Y)\n",
    "        \n",
    "        # delete temp data\n",
    "        del X\n",
    "        \n",
    "# delete temp dir\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Read all the required data from the numpy files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for obtaining the required data for the configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables(config):\n",
    "    \n",
    "    # variables\n",
    "    X_train = np.load(os.path.join(data_path, numpy_dir, \"train_\" + config + \"_X.npy\"), mmap_mode = \"r\")\n",
    "    y_train = np.load(os.path.join(data_path, numpy_dir, \"train_\" + config + \"_Y.npy\"))\n",
    "    \n",
    "    X_test = np.load(os.path.join(data_path, numpy_dir, \"test_\" + config + \"_X.npy\"), mmap_mode = \"r\")\n",
    "    y_test = np.load(os.path.join(data_path, numpy_dir, \"test_\" + config + \"_Y.npy\"))\n",
    "    \n",
    "    X_dev = np.load(os.path.join(data_path, numpy_dir, \"dev_\" + config + \"_X.npy\"), mmap_mode = \"r\")\n",
    "    y_dev = np.load(os.path.join(data_path, numpy_dir, \"dev_\" + config + \"_Y.npy\"))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_dev, y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try reading all data for config: IS09_emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_dev, y_dev = get_variables(\"IS09_emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Define pre-activation residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(input_layer):   # containing: convolution + gated linear unit\n",
    "    \n",
    "    # Obtain convolution layer (1D, temporal convolution) by creating a convolution kernel\n",
    "    tanh_out = Conv1D(NUM_FILTER, \n",
    "                      kernel_size = KERNEL_SIZE,  # length of 1D convolution window = 2\n",
    "                      kernel_initializer= \"random_uniform\",  # initialization of filters\n",
    "                      dilation_rate = DILATION_RATE,\n",
    "                      padding = \"same\")(input_layer)   # Conv1D “depending” on the input layer\n",
    "\n",
    "    # Normalise the result after Conv1D\n",
    "    tanh_out = BatchNormalization()(tanh_out)\n",
    "\n",
    "    # Obtain the convolutional layer with sigmoid transformation (activation)\n",
    "    sigmoid_out = Conv1D(NUM_FILTER, \n",
    "                         kernel_size = KERNEL_SIZE,\n",
    "                         kernel_initializer='random_uniform',\n",
    "                         dilation_rate = DILATION_RATE,\n",
    "                         padding = \"same\")(input_layer)\n",
    "    \n",
    "    # Normalise the layer\n",
    "    sigmoid_out = BatchNormalization()(sigmoid_out)\n",
    "    \n",
    "    # Activation function for the layer\n",
    "    sigmoid_out = Activation(\"sigmoid\")(sigmoid_out)\n",
    "\n",
    "    # Element-wise multiplication\n",
    "    merged = multiply([tanh_out, sigmoid_out])\n",
    "    \n",
    "#     # Max Pooling for the merged result\n",
    "#     merged = MaxPooling1D(pool_size = 2)(merged)\n",
    "    \n",
    "    # Adding input to output and apply relu\n",
    "    merged = add([input_layer, merged])  # addition\n",
    "    gated_cnn = Activation(\"relu\")(merged)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Define a function for generating the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(input_shape):  # input shape: num_files x time x LLD *****\n",
    "    \n",
    "    input_layer = Input(shape = input_shape)  # time x LLD\n",
    "    gated_cnn = residual_block(input_layer)  # first block\n",
    "    \n",
    "    for i in range(0, N_STACK - 1):  # the rest blocks\n",
    "        gated_cnn = residual_block(gated_cnn)\n",
    "    \n",
    "    # Flattening\n",
    "    gated_cnn = Flatten()(gated_cnn)\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    gated_cnn = Dense(256, kernel_initializer = \"random_uniform\")(gated_cnn) \n",
    "    gated_cnn = BatchNormalization()(gated_cnn)\n",
    "    gated_cnn = Activation(\"relu\")(gated_cnn)\n",
    "    gated_cnn = Dropout(0.5)(gated_cnn)\n",
    "    \n",
    "#     # Fully Connected Layer then sigmoid\n",
    "#     gated_cnn = Dense(11, activation = \"softmax\")(gated_cnn)\n",
    "\n",
    "    # Fully Connected Layer then sigmoid\n",
    "    gated_cnn = Dense(7, activation = \"softmax\")(gated_cnn)\n",
    "\n",
    "\n",
    "    all_model = Model(inputs = input_layer, outputs = gated_cnn) # This model will include all layers required in the \n",
    "                                                                 # computation of \"outputs\" given \"inputs\"\n",
    "\n",
    "    all_model.compile(loss = \"categorical_crossentropy\",\n",
    "                      optimizer = \"adam\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "#     all_model.summary()\n",
    "    \n",
    "    return all_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Training a GCNN model with the feature configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for training a GCN model with the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model_name):\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, X_dev, y_dev = get_variables(model_name)\n",
    "    \n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    y_dev = to_categorical(y_dev)\n",
    "    \n",
    "    model = generate_model(X_train.shape[1:])  # model after compilationhttps://www.youtube.com/watch?v=3jWRrafhO7M&start_radio=1&list=RD3jWRrafhO7M\n",
    "    \n",
    "    # fitting training data\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = NUM_EPOCH,\n",
    "              validation_data = (X_dev, y_dev))\n",
    "    \n",
    "    # saving model\n",
    "    model.save(model_name + \"3.model\")  # save model\n",
    "    \n",
    "#     # load model\n",
    "#     new_model = load_model(\"epic_num_reader.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin training model by calling the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTER = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "new_model = load_model(\"IS09_emotion3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = new_model.evaluate(X_test, to_categorical(y_test))\n",
    "print \"loss: \" + str(loss)\n",
    "print \"accuracy: \" + str(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(\"IS09_emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTER = 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(\"IS10_paraling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use 4 categories: anger, excitement(happiness), neutral, and sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading numpy file\n",
    "label_dict = np.load(os.path.join(data_path, \"label_dict.npy\")).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training2(X_train, y_train, X_test, y_test, X_dev, y_dev):\n",
    "    \n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    y_dev = to_categorical(y_dev)\n",
    "    \n",
    "    model = generate_model(X_train.shape[1:])  # model after compilationhttps://www.youtube.com/watch?v=3jWRrafhO7M&start_radio=1&list=RD3jWRrafhO7M\n",
    "    \n",
    "    # fitting training data\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = NUM_EPOCH,\n",
    "              validation_data = (X_dev, y_dev))\n",
    "    \n",
    "    # saving model\n",
    "    model.save(model_name + \"_4_category.model\")  # save model\n",
    "    \n",
    "#     # load model\n",
    "#     new_model = load_model(\"epic_num_reader.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_emo = [0,1,3,4,5]  # combine 1 and 3 latter, converting all 3's to 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = []\n",
    "new_y_train = []\n",
    "for i in range(len(X_train)):\n",
    "    if y_train[i] in selected_emo:\n",
    "        new_X_train.append(X_train[i])\n",
    "        if y_train[i] == 3:\n",
    "            new_y_train.append(1)\n",
    "        else:\n",
    "            new_y_train.append(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
