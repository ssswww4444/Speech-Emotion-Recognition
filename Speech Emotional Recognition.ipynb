{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Speech Emotional Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. OpenSmile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Copying input wav files to data_path/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming all required files are within the \"data\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/peiyun/data\"\n",
    "input_path = os.path.join(data_path, \"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input directory (data_path/input) if not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(input_path):\n",
    "    os.makedirs(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying all input files to input_path. (10,039 utterances in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/peiyun/data/input\n"
     ]
    }
   ],
   "source": [
    "print input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each session\n",
    "for session in range(1,6):\n",
    "    path = os.path.join(data_path, \"IEMOCAP\", \"Session\" + str(session), \"sentences\", \"wav\")\n",
    "    \n",
    "    # for each dialog\n",
    "    for dialog in os.listdir(path):\n",
    "        dialog_path = os.path.join(path, dialog)\n",
    "            \n",
    "        # for each utterance (file)\n",
    "        for filename in os.listdir(dialog_path):\n",
    "            if filename.endswith(\".wav\"):\n",
    "                shutil.copy(os.path.join(dialog_path, filename), os.path.join(input_path, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Getting a dictionary of utterance labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "\n",
    "# for each session\n",
    "for session in range(1,6):\n",
    "    path = os.path.join(data_path, \"IEMOCAP/Session\" + str(session), \"dialog\", \"EmoEvaluation\")\n",
    "    \n",
    "    # for file in the session\n",
    "    for filename in os.listdir(path):\n",
    "        \n",
    "        # only interested in \"summary\" txt files\n",
    "        if filename.endswith(\".txt\"):\n",
    "            f = open(os.path.join(path, filename), \"r\")\n",
    "            for line in f.readlines():\n",
    "                if line[0] == \"[\":\n",
    "                    name, label = line.split(\"\\t\")[1:3]\n",
    "                    label_dict[name] = label\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Split files into training and test sets (70% training, 15% test, 15% dev, seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filename list\n",
    "filename_ls = []\n",
    "for filename in os.listdir(input_path):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        filename_ls.append(filename[:-4])  # [:-4] for removing .wav\n",
    "        \n",
    "# get corresponding label list\n",
    "label_ls = []\n",
    "for filename in filename_ls:\n",
    "    label_ls.append(label_dict[filename])\n",
    "        \n",
    "# splitting into train and test\n",
    "filename_train, filename_remain, label_train, label_remain = train_test_split(filename_ls, label_ls, \n",
    "                                                                          train_size=0.7, random_state=100)\n",
    "\n",
    "# splitting into train and test\n",
    "filename_dev, filename_test, label_dev, label_test = train_test_split(filename_remain, label_remain, \n",
    "                                                                          test_size=0.5, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Writing labels into csv file (7027 train instances, 1506 test instances, 1506 dev instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"label.csv\") , mode='w') as label_file:\n",
    "    writer = csv.writer(label_file, delimiter=\",\")\n",
    "    \n",
    "    # training instances\n",
    "    for filename in filename_train:\n",
    "        writer.writerow([filename, \"train\", label_dict[filename]])\n",
    "            \n",
    "    # test instances\n",
    "    for filename in filename_test:\n",
    "        writer.writerow([filename, \"test\", label_dict[filename]])\n",
    "        \n",
    "    # test instances\n",
    "    for filename in filename_dev:\n",
    "        writer.writerow([filename, \"dev\", label_dict[filename]])\n",
    "                \n",
    "    label_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Move files into test, dev, and train directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test and train directory for input instances if not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "    path = os.path.join(input_path, data_type)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving files to its directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filename_train:\n",
    "    shutil.move(os.path.join(input_path, filename + \".wav\"), os.path.join(input_path, \"train\", filename + \".wav\"))\n",
    "    \n",
    "for filename in filename_test:\n",
    "    shutil.move(os.path.join(input_path, filename + \".wav\"), os.path.join(input_path, \"test\", filename + \".wav\"))\n",
    "    \n",
    "for filename in filename_dev:\n",
    "    shutil.move(os.path.join(input_path, filename + \".wav\"), os.path.join(input_path, \"dev\", filename + \".wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction with openSMILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File for feature extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting feature.py\n"
     ]
    }
   ],
   "source": [
    "%%file feature.py\n",
    "\n",
    "# Import the required modules\n",
    "import argparse\n",
    "import os\n",
    "from subprocess import call\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "from time import gmtime, strftime, time\n",
    "\n",
    "# Global variables\n",
    "data_path = \"/work/peiyun/data\"\n",
    "\n",
    "# Get the ground_truth label number of the file\n",
    "def get_label(label_file, filename):\n",
    "    \n",
    "    with open(label_file, mode = \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            name, data_type, label = row\n",
    "            if name == filename:\n",
    "                return label\n",
    "\n",
    "# Check input and output directories\n",
    "def check_dirs(args):\n",
    "    \n",
    "    # Check input directory (if not exist -> error)\n",
    "    if not os.path.exists(os.path.join(data_path, args.input_dir)):\n",
    "        print \"Error: input directory not exist\"\n",
    "        return False\n",
    "    for data_type in [\"train\", \"test\"]:\n",
    "        path = os.path.join(data_path, args.input_dir, data_type)\n",
    "        if not (os.path.exists(path)):\n",
    "            print \"Error: input directory missing train or test directories\"\n",
    "            return False\n",
    "    \n",
    "    # Check output directory (if not exist -> create one)\n",
    "    if not os.path.exists(os.path.join(data_path, args.output_dir)):\n",
    "        os.makedirs(os.path.join(data_path, args.output_dir))\n",
    "    for data_type in [\"train\", \"test\"]:\n",
    "        path = os.path.join(data_path, args.output_dir, data_type)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Function for extracting features with openSMILE (Return whether successed)\n",
    "def extract_features(args):\n",
    "    if not check_dirs(args):\n",
    "        return False   # failed to read inputs\n",
    "            \n",
    "    # Iterate over wav audio files in input directory\n",
    "    for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "        path_in = os.path.join(data_path, args.input_dir, data_type)\n",
    "        \n",
    "        for filename in os.listdir(path_in):\n",
    "            \n",
    "            # Only interested in wav files\n",
    "            if filename.endswith(\".wav\"):\n",
    "                # in\n",
    "                file_in = os.path.join(path_in, filename)\n",
    "                config = os.path.join(data_path, \"config\", args.config)\n",
    "                \n",
    "                filename = filename[:-4]  # [:-4] for removing .wav\n",
    "                \n",
    "                # out\n",
    "                path_out = os.path.join(data_path, args.output_dir, data_type)\n",
    "                csv_out = os.path.join(path_out, filename + \"_\" + args.config[:-5] + \".csv\")\n",
    "                arff_out = os.path.join(path_out, filename + \"_\" + args.config[:-5] + \".arff\")  # [:-5] for removing .conf\n",
    "                label = get_label(os.path.join(data_path, args.label), filename)\n",
    "                \n",
    "                # use openSMILE\n",
    "                call([\"SMILExtract\", \"-l\", \"0\", \"-noconsoleoutput\", \"-I\", file_in, \n",
    "                      \"-C\", config, \"-D\", csv_out, \"-O\", arff_out, \"-instname\", filename, \"-label\", label])\n",
    "                \n",
    "    return True\n",
    "\n",
    "# Obtaining args from terminal\n",
    "def get_args():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Extract features for files in the directory using openSMILE')\n",
    "    \n",
    "    parser.add_argument(\"-i\",                   # optional argument (no \"-\" for positional)\n",
    "                        \"--input_dir\",   # name of the attribute (dest)\n",
    "                        action = \"store\",       # can be \"store\", \"store_const\", \"store_true\", etc.\n",
    "                        # nargs = N for associating N args with a single action\n",
    "                        # const = ... to hold constant values\n",
    "                        # default = ... to set default value\n",
    "                        type = str,             # check arg type\n",
    "                        # choice = [.., .., ..] # restrict set of values\n",
    "                        required = True,        # make an option required\n",
    "                        # metavar = \"XXX\" for changing display name\n",
    "                        help = \"The directory of input audio files (wav)\")\n",
    "    \n",
    "    parser.add_argument(\"-o\", \"--output_dir\", type = str, required = True, help = \"The directory of results\")\n",
    "    parser.add_argument(\"-c\", \"--config\", type = str, required = True, help = \"Configuration filename\")\n",
    "    parser.add_argument(\"-l\", \"--label\", type = str, required = True, help = \"Label filename\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    # Obtaining terminal args\n",
    "    args = get_args()\n",
    "    \n",
    "    start_time = time()\n",
    "    \n",
    "    # Extracting features according to args\n",
    "    if not extract_features(args):\n",
    "        print \"Failed to extract features\"\n",
    "    else:\n",
    "        end_time = time()\n",
    "        print(\"Time taken for extracting features:\", strftime(\"%H:%M:%S\", gmtime(end_time - start_time)))\n",
    "        print \"Successfully extracted features\"\n",
    "\n",
    "# If running the file directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running script for extracting features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%!\n",
    "python feature.py -i \"input\" -o \"output\" -c \"IS09_emotion.conf\" -l \"label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('Time taken for extracting features:', '00:35:29')\",\n",
       " 'Successfully extracted features']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%!\n",
    "python feature.py -i \"input\" -o \"output\" -c \"IS10_paraling.conf\" -l \"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Neural Network for emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training, development, and test sets in the format that is suitable for neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ground_truth label number of the file\n",
    "def get_label(filename):\n",
    "    label_file = os.path.join(data_path, \"label.csv\")\n",
    "    \n",
    "    with open(label_file, mode = \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            name, data_type, label = row\n",
    "            if name == filename:\n",
    "                return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_Xy(file_path):\n",
    "    \n",
    "    # Obtain dataframe for the csv file (one utterance/instance)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print type(df)\n",
    "    \n",
    "    print df.columns[1]\n",
    "\n",
    "    # Get filename\n",
    "    filename = df.loc[\"name\",]\n",
    "    \n",
    "    print filename\n",
    "    \n",
    "    # Clean unnecessary columns\n",
    "    df = df.drop(columns = [\"name\", \"frameTime\"])\n",
    "    \n",
    "    label = get_label(filename)\n",
    "    \n",
    "    print df\n",
    "    \n",
    "    print label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-64b00650580a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Ses05M_script01_2_F000_IS10_paraling.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mextract_Xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-162-b0204642a78f>\u001b[0m in \u001b[0;36mextract_Xy\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Get filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/peiyun/anaconda2/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(data_path, \"output\", \"train\", \"Ses05M_script01_2_F000_IS10_paraling.csv\")\n",
    "extract_Xy(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(12).reshape(3,4),columns=['A', 'B', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"A\"].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B   C   D\n",
       "0  0  1   2   3\n",
       "1  4  5   6   7\n",
       "2  8  9  10  11"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model_name, fold,\n",
    "             x_train, y_train,\n",
    "             x_dev, y_dev,\n",
    "             x_test, y_test,\n",
    "             batch_size,\n",
    "             nb_epoch,\n",
    "             validation_split,\n",
    "             params_path):\n",
    "    params = GCNN_Params(params_path)\n",
    "    if fold > 0:\n",
    "        model_name = os.path.join(model_name, str(fold))\n",
    "\n",
    "    \"\"\" Training process of GCNN \"\"\"\n",
    "    print('\\nBuilding GCNN...')\n",
    "    all_model = generate_model(params.is_dilated,\n",
    "                               params.nb_filter,\n",
    "                               params.dilation_rate,\n",
    "                               params.n_stack,\n",
    "                               x_train.shape)\n",
    "\n",
    "    print('\\nCompile all_model...')\n",
    "    all_model.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy', precision, recall, fscore])\n",
    "    all_model.summary()\n",
    "\n",
    "    best_model_filepath = os.path.join(\"models\", model_name +\".best.hdf5\")\n",
    "    if not os.path.exists(os.path.dirname(best_model_filepath)):\n",
    "        os.makedirs(os.path.dirname(best_model_filepath))\n",
    "    tensorboard_path = os.path.join(\"logs\", \"tensorboard\", model_name)\n",
    "    if not os.path.exists(tensorboard_path):\n",
    "        os.makedirs(tensorboard_path)\n",
    "    csv_logpath = os.path.join(\"logs\", \"csv\", model_name + '.log.csv')\n",
    "    if not os.path.exists(os.path.dirname(csv_logpath)):\n",
    "        os.makedirs(os.path.dirname(csv_logpath))\n",
    "\n",
    "    # Create all callback instances\n",
    "    # Early stop is not used\n",
    "    early_stop = EarlyStopping(monitor='val_acc', patience=5, mode='max', verbose=1)\n",
    "    checkpoint_best = ModelCheckpoint(best_model_filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    csv_logger = CSVLogger(csv_logpath)\n",
    "    tensorboard = TensorBoard(log_dir=tensorboard_path, histogram_freq=0, write_graph=True, write_images=True)\n",
    "    callbacks_list = [tensorboard, metrics, checkpoint_best, csv_logger] # [metrics, checkpoint_best, tensorboard, early_stop]\n",
    "\n",
    "    print('\\nFitting all_model...')\n",
    "\n",
    "    if validation_split > 0:\n",
    "        print(\"Fitting all_model with validation split...   \", validation_split)\n",
    "        history = all_model.fit(x_train, y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=nb_epoch,\n",
    "                                validation_split=validation_split,\n",
    "                                callbacks=callbacks_list,\n",
    "                                shuffle=True)\n",
    "    elif x_dev is not None and y_dev is not None:\n",
    "        print(\"Fitting all_model with validation set...\")\n",
    "        history = all_model.fit(x_train, y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=nb_epoch,\t\n",
    "                                validation_data=(x_dev, y_dev),\n",
    "                                callbacks=callbacks_list,\n",
    "                                shuffle=True)\n",
    "    elif x_test is not None and y_test is not None:\n",
    "        print(\"Fitting all_model using test set...\")\n",
    "        history = all_model.fit(x_train, y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=nb_epoch,\n",
    "                                validation_data=(x_test, y_test),\n",
    "                                callbacks=callbacks_list,\n",
    "                                shuffle=True)\n",
    "\n",
    "    last_model_filepath = os.path.join(\"models\", model_name +\".last.hdf5\")\n",
    "    all_model.save(last_model_filepath)\n",
    "\n",
    "    # Save model plot\n",
    "    if fold > 0:\n",
    "        model_plot_name = os.path.join(\"models\", \"/\".join(model_name.split(\"/\")[:-1]), \"structure.png\") # take only model name, without fold\n",
    "    else:\n",
    "        model_plot_name = os.path.join(\"models\", model_name + \".structure.png\") # take only model name, without fold\n",
    "    if not os.path.exists(os.path.dirname(model_plot_name)):\n",
    "        os.makedirs(os.path.dirname(model_plot_name))\n",
    "    plot_model(all_model, to_file= model_plot_name)\n",
    "\n",
    "    acc = 0\n",
    "    if x_test is not None and y_test is not None:\n",
    "        print(\"\\nEvaluate all_model\")\n",
    "        print(\"\\# Best all_model based on val set\")\n",
    "        all_model.load_weights(best_model_filepath)\n",
    "        score = all_model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "        print(score)\n",
    "        y_pred = np.round(all_model.predict(x_test, batch_size=batch_size))\n",
    "        print(y_test.shape)\n",
    "        print(np.squeeze(y_pred).shape)\n",
    "        _val_f1 = f1_score(y_test, y_pred)\n",
    "        _val_recall = recall_score(y_test, y_pred)\n",
    "        _val_precision = precision_score(y_test, y_pred)\n",
    "        TN, FP, FN, TP = confusion_matrix(y_test, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        print_cm(TP, FP, FN, TN, [\"Control\",\"Dementia\"])\n",
    "        UAR, WAR, fscore_0, fscore_1, uaf, waf = get_other_metrics(TP, FP, FN, TN)\n",
    "        print(\"test_acc: %.4f | test_uar: %.4f - test_war: %.4f | - test_fscore0: %.4f - test_fscore1: %.4f - test_uaf: %.4f - test_waf: %.4f\" % (acc, UAR, WAR, fscore_0, fscore_1, uaf, waf))\n",
    "        log = TrainingLog(csv_logpath)\n",
    "        log.print()\n",
    "        return np.array([y_test.tolist(), np.squeeze(y_pred).tolist()]).transpose().tolist(), acc, uaf, waf\n",
    "    else:\n",
    "        return None, None, None, None\n",
    "    print(\"Training finished. Log can be found in folder\", csv_logpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LEN = 22500 # 22499 \n",
    "DEF_BATCH_SIZE = 4\n",
    "DEF_NB_EPOCH = 32\n",
    "DEF_VALIDATION_SPLIT = 0\n",
    "SHUFFLE = True\n",
    "DEFAULT_THRESHOLD = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
