{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Speech Emotional Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. OpenSmile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Copying input wav files to data_path/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming all required files are within the \"data\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/peiyun/data\"\n",
    "input_path = os.path.join(data_path, \"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input directory (data_path/input) if not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(input_path):\n",
    "    os.makedirs(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying all input files to input_path. (10,039 utterances in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/peiyun/data/input\n"
     ]
    }
   ],
   "source": [
    "print input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each session\n",
    "for session in range(1,6):\n",
    "    path = os.path.join(data_path, \"IEMOCAP\", \"Session\" + str(session), \"sentences\", \"wav\")\n",
    "    \n",
    "    # for each dialog\n",
    "    for dialog in os.listdir(path):\n",
    "        dialog_path = os.path.join(path, dialog)\n",
    "            \n",
    "        # for each utterance (file)\n",
    "        for filename in os.listdir(dialog_path):\n",
    "            if filename.endswith(\".wav\"):\n",
    "                shutil.copy(os.path.join(dialog_path, filename), os.path.join(input_path, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Getting a dictionary of utterance labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "\n",
    "# for each session\n",
    "for session in range(1,6):\n",
    "    path = os.path.join(data_path, \"IEMOCAP/Session\" + str(session), \"dialog\", \"EmoEvaluation\")\n",
    "    \n",
    "    # for file in the session\n",
    "    for filename in os.listdir(path):\n",
    "        \n",
    "        # only interested in \"summary\" txt files\n",
    "        if filename.endswith(\".txt\"):\n",
    "            f = open(os.path.join(path, filename), \"r\")\n",
    "            for line in f.readlines():\n",
    "                if line[0] == \"[\":\n",
    "                    name, label = line.split(\"\\t\")[1:3]\n",
    "                    label_dict[name] = label\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Split files into training and test sets (70% training, 15% test, 15% dev, seed = 100)\n",
    "(seed = 100 for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filename list\n",
    "filename_ls = []\n",
    "for filename in os.listdir(input_path):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        filename_ls.append(filename[:-4])  # [:-4] for removing .wav\n",
    "        \n",
    "# get corresponding label list\n",
    "label_ls = []\n",
    "for filename in filename_ls:\n",
    "    label_ls.append(label_dict[filename])\n",
    "        \n",
    "# splitting into train and test\n",
    "filename_train, filename_remain, label_train, label_remain = train_test_split(filename_ls, label_ls, \n",
    "                                                                          train_size=0.7, random_state=100)\n",
    "\n",
    "# splitting into train and test\n",
    "filename_dev, filename_test, label_dev, label_test = train_test_split(filename_remain, label_remain, \n",
    "                                                                          test_size=0.5, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Writing labels into csv file (7027 train instances, 1506 test instances, 1506 dev instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"label.csv\") , mode='w') as label_file:\n",
    "    writer = csv.writer(label_file, delimiter=\",\")\n",
    "    \n",
    "    # training instances\n",
    "    for filename in filename_train:\n",
    "        writer.writerow([filename, \"train\", label_dict[filename]])\n",
    "            \n",
    "    # test instances\n",
    "    for filename in filename_test:\n",
    "        writer.writerow([filename, \"test\", label_dict[filename]])\n",
    "        \n",
    "    # test instances\n",
    "    for filename in filename_dev:\n",
    "        writer.writerow([filename, \"dev\", label_dict[filename]])\n",
    "                \n",
    "    label_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Move files into test, dev, and train directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test and train directory for input instances if not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "    path = os.path.join(input_path, data_type)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving files to its directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filename_train:\n",
    "    shutil.move(os.path.join(input_path, filename + \".wav\"), os.path.join(input_path, \"train\", filename + \".wav\"))\n",
    "    \n",
    "for filename in filename_test:\n",
    "    shutil.move(os.path.join(input_path, filename + \".wav\"), os.path.join(input_path, \"test\", filename + \".wav\"))\n",
    "    \n",
    "for filename in filename_dev:\n",
    "    shutil.move(os.path.join(input_path, filename + \".wav\"), os.path.join(input_path, \"dev\", filename + \".wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction with openSMILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File for feature extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting feature.py\n"
     ]
    }
   ],
   "source": [
    "%%file feature.py\n",
    "\n",
    "# Import the required modules\n",
    "import argparse\n",
    "import os\n",
    "from subprocess import call\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "from time import gmtime, strftime, time\n",
    "\n",
    "# Global variables\n",
    "data_path = \"/work/peiyun/data\"\n",
    "\n",
    "# Get the ground_truth label number of the file\n",
    "def get_label(label_file, filename):\n",
    "    \n",
    "    with open(label_file, mode = \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            name, data_type, label = row\n",
    "            if name == filename:\n",
    "                return label\n",
    "\n",
    "# Check input and output directories\n",
    "def check_dirs(args):\n",
    "    \n",
    "    # Check input directory (if not exist -> error)\n",
    "    if not os.path.exists(os.path.join(data_path, args.input_dir)):\n",
    "        print \"Error: input directory not exist\"\n",
    "        return False\n",
    "    for data_type in [\"train\", \"test\"]:\n",
    "        path = os.path.join(data_path, args.input_dir, data_type)\n",
    "        if not (os.path.exists(path)):\n",
    "            print \"Error: input directory missing train or test directories\"\n",
    "            return False\n",
    "    \n",
    "    # Check output directory (if not exist -> create one)\n",
    "    if not os.path.exists(os.path.join(data_path, args.output_dir)):\n",
    "        os.makedirs(os.path.join(data_path, args.output_dir))\n",
    "    for data_type in [\"train\", \"test\"]:\n",
    "        path = os.path.join(data_path, args.output_dir, data_type)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Function for extracting features with openSMILE (Return whether successed)\n",
    "def extract_features(args):\n",
    "    if not check_dirs(args):\n",
    "        return False   # failed to read inputs\n",
    "            \n",
    "    # Iterate over wav audio files in input directory\n",
    "    for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "        path_in = os.path.join(data_path, args.input_dir, data_type)\n",
    "        \n",
    "        for filename in os.listdir(path_in):\n",
    "            \n",
    "            # Only interested in wav files\n",
    "            if filename.endswith(\".wav\"):\n",
    "                # in\n",
    "                file_in = os.path.join(path_in, filename)\n",
    "                config = os.path.join(data_path, \"config\", args.config)\n",
    "                \n",
    "                filename = filename[:-4]  # [:-4] for removing .wav\n",
    "                \n",
    "                # out\n",
    "                path_out = os.path.join(data_path, args.output_dir, data_type)\n",
    "                csv_out = os.path.join(path_out, filename + \"_\" + args.config[:-5] + \".csv\")\n",
    "                arff_out = os.path.join(path_out, filename + \"_\" + args.config[:-5] + \".arff\")  # [:-5] for removing .conf\n",
    "                label = get_label(os.path.join(data_path, args.label), filename)\n",
    "                \n",
    "                # use openSMILE\n",
    "                call([\"SMILExtract\", \"-l\", \"0\", \"-noconsoleoutput\", \"-I\", file_in, \n",
    "                      \"-C\", config, \"-D\", csv_out, \"-O\", arff_out, \"-instname\", filename, \"-label\", label])\n",
    "                \n",
    "    return True\n",
    "\n",
    "# Obtaining args from terminal\n",
    "def get_args():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Extract features for files in the directory using openSMILE')\n",
    "    \n",
    "    parser.add_argument(\"-i\",                   # optional argument (no \"-\" for positional)\n",
    "                        \"--input_dir\",   # name of the attribute (dest)\n",
    "                        action = \"store\",       # can be \"store\", \"store_const\", \"store_true\", etc.\n",
    "                        # nargs = N for associating N args with a single action\n",
    "                        # const = ... to hold constant values\n",
    "                        # default = ... to set default value\n",
    "                        type = str,             # check arg type\n",
    "                        # choice = [.., .., ..] # restrict set of values\n",
    "                        required = True,        # make an option required\n",
    "                        # metavar = \"XXX\" for changing display name\n",
    "                        help = \"The directory of input audio files (wav)\")\n",
    "    \n",
    "    parser.add_argument(\"-o\", \"--output_dir\", type = str, required = True, help = \"The directory of results\")\n",
    "    parser.add_argument(\"-c\", \"--config\", type = str, required = True, help = \"Configuration filename\")\n",
    "    parser.add_argument(\"-l\", \"--label\", type = str, required = True, help = \"Label filename\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    # Obtaining terminal args\n",
    "    args = get_args()\n",
    "    \n",
    "    start_time = time()\n",
    "    \n",
    "    # Extracting features according to args\n",
    "    if not extract_features(args):\n",
    "        print \"Failed to extract features\"\n",
    "    else:\n",
    "        end_time = time()\n",
    "        print(\"Time taken for extracting features:\", strftime(\"%H:%M:%S\", gmtime(end_time - start_time)))\n",
    "        print \"Successfully extracted features\"\n",
    "\n",
    "# If running the file directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running script for extracting features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%!\n",
    "python feature.py -i \"input\" -o \"output\" -c \"IS09_emotion.conf\" -l \"label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('Time taken for extracting features:', '00:35:29')\",\n",
       " 'Successfully extracted features']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%!\n",
    "python feature.py -i \"input\" -o \"output\" -c \"IS10_paraling.conf\" -l \"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gated Convolutional Network for emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Set parameters for GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LEN = 22500 # 22499 (for each utterance)\n",
    "DEF_BATCH_SIZE = 4\n",
    "DEF_NB_EPOCH = 32\n",
    "DEF_VALIDATION_SPLIT = 0\n",
    "SHUFFLE = True\n",
    "DEFAULT_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Extracting the matrices and labels for all data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for obtaining label according to the label.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ground_truth label number of the file\n",
    "def get_label(filename):\n",
    "    label_file = os.path.join(data_path, \"label.csv\")\n",
    "    \n",
    "    with open(label_file, mode = \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            name, data_type, label = row\n",
    "            if name == filename:\n",
    "                return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for extracting the corresponding normalised input and the ground truth output of a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_Xy(file_path):\n",
    "    \n",
    "    # Obtain dataframe for the csv file (one utterance/instance)\n",
    "    df = pd.read_csv(file_path, sep = \";\")\n",
    "    \n",
    "    # Get filename\n",
    "    filename = df[\"name\"][0]\n",
    "    \n",
    "    # Clean unnecessary columns\n",
    "    df = df.drop(columns = [\"name\", \"frameTime\"])\n",
    "    \n",
    "    # Normalise data\n",
    "    x = df.values                # dataframe to a numpy array\n",
    "    min_max_scalar = MinMaxScaler()  \n",
    "    x_scaled = min_max_scalar.fit_transform(x)     # scaling each feature (each column) to range: (0,1)\n",
    "    df = pd.DataFrame(x_scaled)  # numpy array back to dataframe\n",
    "    \n",
    "    # Obtain matrix X and label y\n",
    "    x_data = sequence.pad_sequences(df.values.T, padding='post', dtype='float64', maxlen=MAX_LEN).T\n",
    "    y_data = get_label(filename)\n",
    "    \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = os.path.join(\"/Users/peiyuns/Desktop/mydata\", \"Ses01F_impro06_F021_IS10_paraling.csv\")\n",
    "# print extract_Xy(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the required matrix and label pairs for each data type and store as numpy files. \n",
    "<br>\n",
    "NOTE: np.load(filename.npy) to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "    X = []\n",
    "    Y = []\n",
    "    path = os.path.join(data_path, \"output\", data_type)\n",
    "    for filename in os.listdir(path):\n",
    "        x_data, y_data = extract_Xy(os.path.join(path, filename))\n",
    "        X.append(x_data)\n",
    "        Y.append(y_data)\n",
    "        \n",
    "    # save as numpy file\n",
    "    np.save(data_type + \"_X.npy\", X)\n",
    "    np.save(data_type + \"_Y.npy\", Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the required data for each data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = np.load(\"train_X.npy\")\n",
    "y_train = np.load(\"train_Y.npy\")\n",
    "\n",
    "# test\n",
    "X_test = np.load(\"test_X.npy\")\n",
    "y_test = np.load(\"test_Y.npy\")\n",
    "\n",
    "# dev\n",
    "X_dev = np.load(\"dev_X.npy\")\n",
    "y_dev = np.load(\"dev_Y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the model for Gated Convolutional Network. (1-dimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    \"\"\" Generate model GCNN \"\"\"\n",
    "    input_ = Input(shape=(input_shape[1], input_shape[2]))\n",
    "    gated_dilcnn = gated_block(1, nb_filter, 1)(input_)\n",
    "\n",
    "\n",
    "    for i in range(0, n_stack - 1):\n",
    "        block = int((i+1)/(n_stack+1)) + 1\n",
    "        if not is_dilated:\n",
    "            block = str(block) + \"_\" + str(i+1)\n",
    "        else:\n",
    "            block = str(block)\n",
    "        dil = dilation_rate**((i+1)%(n_stack+1))\n",
    "        \n",
    "        print(block)\n",
    "        gated_dilcnn = gated_block(block, nb_filter, dil)(gated_dilcnn)\n",
    "\n",
    "    gated_dilcnn = Flatten()(gated_dilcnn)\n",
    "    gated_dilcnn = Dense(256, kernel_initializer='random_uniform')(gated_dilcnn) \n",
    "    gated_dilcnn = BatchNormalization()(gated_dilcnn)\n",
    "    gated_dilcnn = Activation('relu')(gated_dilcnn)\n",
    "    \n",
    "    gated_dilcnn = Dropout(0.5)(gated_dilcnn)\n",
    "    gated_dilcnn = Dense(1, activation='sigmoid')(gated_dilcnn)\n",
    "\n",
    "    all_model = Model(input_, gated_dilcnn)\n",
    "\n",
    "    print('\\nCompile gated_dilcnn...')\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    all_model.compile(loss='binary_crossentropy',\n",
    "            optimizer='sgd',\n",
    "            metrics=['accuracy', precision, recall, fscore])\n",
    "    all_model.summary()\n",
    "    return all_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for training a GCN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin training model by calling the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(args.model_name, 0,\n",
    "         x_train, y_train,\n",
    "         x_dev, y_dev,\n",
    "         x_test, y_test,\n",
    "         DEF_BATCH_SIZE,\n",
    "         DEF_NB_EPOCH,\n",
    "         args.split,\n",
    "         args.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
