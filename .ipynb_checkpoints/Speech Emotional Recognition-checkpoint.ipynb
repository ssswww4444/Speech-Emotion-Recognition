{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Speech Emotional Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. OpenSmile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Copying input wav files to data_path/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming all required files are within the \"data\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/peiyun/data\"\n",
    "input_path = os.path.join(data_path, \"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input directory (data_path/input) if not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(input_path):\n",
    "    os.makedirs(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying all input files to input_path. (10,039 utterances in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/peiyun/data/input\n"
     ]
    }
   ],
   "source": [
    "print input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each session\n",
    "for session in range(1,6):\n",
    "    path = os.path.join(data_path, \"IEMOCAP\", \"Session\" + str(session), \"sentences\", \"wav\")\n",
    "    \n",
    "    # for each dialog\n",
    "    for dialog in os.listdir(path):\n",
    "        dialog_path = os.path.join(path, dialog)\n",
    "            \n",
    "        # for each utterance (file)\n",
    "        for filename in os.listdir(dialog_path):\n",
    "            if filename.endswith(\".wav\"):\n",
    "                shutil.copy(os.path.join(dialog_path, filename), os.path.join(input_path, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Getting a dictionary of utterance labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "\n",
    "# for each session\n",
    "for session in range(1,6):\n",
    "    path = os.path.join(data_path, \"IEMOCAP/Session\" + str(session), \"dialog\", \"EmoEvaluation\")\n",
    "    \n",
    "    # for file in the session\n",
    "    for filename in os.listdir(path):\n",
    "        \n",
    "        # only interested in \"summary\" txt files\n",
    "        if filename.endswith(\".txt\"):\n",
    "            f = open(os.path.join(path, filename), \"r\")\n",
    "            for line in f.readlines():\n",
    "                if line[0] == \"[\":\n",
    "                    name, label = line.split(\"\\t\")[1:3]\n",
    "                    label_dict[name] = label\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Split files into training and test sets (70% training, 15% test, 15% dev, seed = 100)\n",
    "(seed = 100 for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filename list\n",
    "filename_ls = []\n",
    "for filename in os.listdir(input_path):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        filename_ls.append(filename[:-4])  # [:-4] for removing .wav\n",
    "        \n",
    "# get corresponding label list\n",
    "label_ls = []\n",
    "for filename in filename_ls:\n",
    "    label_ls.append(label_dict[filename])\n",
    "        \n",
    "# splitting into train and test\n",
    "filename_train, filename_remain, label_train, label_remain = train_test_split(filename_ls, label_ls, \n",
    "                                                                              train_size=0.7, random_state=100,\n",
    "                                                                              shuffle = True)\n",
    "\n",
    "# splitting into train and test\n",
    "filename_dev, filename_test, label_dev, label_test = train_test_split(filename_remain, label_remain, \n",
    "                                                                      test_size=0.5, random_state=100,\n",
    "                                                                      shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Writing labels into csv file (7027 train instances, 1506 test instances, 1506 dev instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, \"label.csv\") , mode='w') as label_file:\n",
    "    writer = csv.writer(label_file, delimiter=\",\")\n",
    "    \n",
    "    # training instances\n",
    "    for filename in filename_train:\n",
    "        writer.writerow([filename, \"train\", label_dict[filename]])\n",
    "            \n",
    "    # test instances\n",
    "    for filename in filename_test:\n",
    "        writer.writerow([filename, \"test\", label_dict[filename]])\n",
    "        \n",
    "    # test instances\n",
    "    for filename in filename_dev:\n",
    "        writer.writerow([filename, \"dev\", label_dict[filename]])\n",
    "                \n",
    "    label_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Move files into test, dev, and train directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test and train directory for input instances if not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "    path = os.path.join(input_path, data_type)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving files to its directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filename_train:\n",
    "    shutil.move(os.path.join(input_path, filename + \".wav\"), os.path.join(input_path, \"train\", filename + \".wav\"))\n",
    "    \n",
    "for filename in filename_test:\n",
    "    shutil.move(os.path.join(input_path, filename + \".wav\"), os.path.join(input_path, \"test\", filename + \".wav\"))\n",
    "    \n",
    "for filename in filename_dev:\n",
    "    shutil.move(os.path.join(input_path, filename + \".wav\"), os.path.join(input_path, \"dev\", filename + \".wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction with openSMILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File for feature extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting feature.py\n"
     ]
    }
   ],
   "source": [
    "%%file feature.py\n",
    "\n",
    "# Import the required modules\n",
    "import argparse\n",
    "import os\n",
    "from subprocess import call\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "from time import gmtime, strftime, time\n",
    "\n",
    "# Global variables\n",
    "data_path = \"/work/peiyun/data\"\n",
    "\n",
    "# Get the ground_truth label number of the file\n",
    "def get_label(label_file, filename):\n",
    "    \n",
    "    with open(label_file, mode = \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            name, data_type, label = row\n",
    "            if name == filename:\n",
    "                return label\n",
    "\n",
    "# Check input and output directories\n",
    "def check_dirs(args):\n",
    "    \n",
    "    # Check input directory (if not exist -> error)\n",
    "    if not os.path.exists(os.path.join(data_path, args.input_dir)):\n",
    "        print \"Error: input directory not exist\"\n",
    "        return False\n",
    "    for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "        path = os.path.join(data_path, args.input_dir, data_type)\n",
    "        if not (os.path.exists(path)):\n",
    "            print \"Error: input directory missing train or test directories\"\n",
    "            return False\n",
    "    \n",
    "    # Check output directory (if not exist -> create one)\n",
    "    if not os.path.exists(os.path.join(data_path, args.output_dir)):\n",
    "        os.makedirs(os.path.join(data_path, args.output_dir))\n",
    "    for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "        path = os.path.join(data_path, args.output_dir, data_type)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        config_path = os.path.join(path, args.config[:-5])\n",
    "        if not os.path.exists(config_path):\n",
    "            os.makedirs(config_path)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Function for extracting features with openSMILE (Return whether successed)\n",
    "def extract_features(args):\n",
    "    if not check_dirs(args):\n",
    "        return False   # failed to read inputs\n",
    "            \n",
    "    # Iterate over wav audio files in input directory\n",
    "    for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "        path_in = os.path.join(data_path, args.input_dir, data_type)\n",
    "        \n",
    "        for filename in os.listdir(path_in):\n",
    "            \n",
    "            # Only interested in wav files\n",
    "            if filename.endswith(\".wav\"):\n",
    "                # in\n",
    "                file_in = os.path.join(path_in, filename)\n",
    "                config = os.path.join(data_path, \"config\", args.config)\n",
    "                \n",
    "                filename = filename[:-4]  # [:-4] for removing .wav\n",
    "                \n",
    "                # out\n",
    "                path_out = os.path.join(data_path, args.output_dir, data_type, args.config[:-5])\n",
    "                csv_out = os.path.join(path_out, filename + \"_\" + args.config[:-5] + \".csv\")\n",
    "                arff_out = os.path.join(path_out, filename + \"_\" + args.config[:-5] + \".arff\")  # [:-5] for removing .conf\n",
    "                label = get_label(os.path.join(data_path, args.label), filename)\n",
    "                \n",
    "                # use openSMILE\n",
    "                call([\"SMILExtract\", \"-l\", \"0\", \"-noconsoleoutput\", \"-I\", file_in, \n",
    "                      \"-C\", config, \"-D\", csv_out, \"-O\", arff_out, \"-instname\", filename, \"-label\", label])\n",
    "                \n",
    "    return True\n",
    "\n",
    "# Obtaining args from terminal\n",
    "def get_args():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Extract features for files in the directory using openSMILE')\n",
    "    \n",
    "    parser.add_argument(\"-i\",                   # optional argument (no \"-\" for positional)\n",
    "                        \"--input_dir\",   # name of the attribute (dest)\n",
    "                        action = \"store\",       # can be \"store\", \"store_const\", \"store_true\", etc.\n",
    "                        # nargs = N for associating N args with a single action\n",
    "                        # const = ... to hold constant values\n",
    "                        # default = ... to set default value\n",
    "                        type = str,             # check arg type\n",
    "                        # choice = [.., .., ..] # restrict set of values\n",
    "                        required = True,        # make an option required\n",
    "                        # metavar = \"XXX\" for changing display name\n",
    "                        help = \"The directory of input audio files (wav)\")\n",
    "    \n",
    "    parser.add_argument(\"-o\", \"--output_dir\", type = str, required = True, help = \"The directory of results\")\n",
    "    parser.add_argument(\"-c\", \"--config\", type = str, required = True, help = \"Configuration filename\")\n",
    "    parser.add_argument(\"-l\", \"--label\", type = str, required = True, help = \"Label filename\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    # Obtaining terminal args\n",
    "    args = get_args()\n",
    "    \n",
    "    start_time = time()\n",
    "    \n",
    "    # Extracting features according to args\n",
    "    if not extract_features(args):\n",
    "        print \"Failed to extract features\"\n",
    "    else:\n",
    "        end_time = time()\n",
    "        print(\"Time taken for extracting features:\", strftime(\"%H:%M:%S\", gmtime(end_time - start_time)))\n",
    "        print \"Successfully extracted features\"\n",
    "\n",
    "# If running the file directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running script for extracting features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('Time taken for extracting features:', '00:00:00')\",\n",
       " 'Successfully extracted features']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%!\n",
    "python feature.py -i \"input\" -o \"output\" -c \"IS09_emotion.conf\" -l \"label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('Time taken for extracting features:', '00:35:29')\",\n",
       " 'Successfully extracted features']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%!\n",
    "python feature.py -i \"input\" -o \"output\" -c \"IS10_paraling.conf\" -l \"label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gated Convolutional Network for emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peiyuns/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Conv1D, Input, MaxPooling1D\n",
    "from keras.layers import add, multiply\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "import os\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/work/peiyun/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Set parameters for GCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LEN = 22500 # 22499 (for each utterance)\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCH = 32\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "IS_DILATED = False\n",
    "NUM_FILTER = 64\n",
    "DILATION_RATE = 1\n",
    "N_STACK = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Extracting the matrices and labels for all data types and store as numpy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for obtaining label according to the label.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ground_truth label number of the file\n",
    "def get_label(filename):\n",
    "    label_file = os.path.join(data_path, \"label.csv\")\n",
    "    \n",
    "    with open(label_file, mode = \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            name, data_type, label = row\n",
    "            # print filename\n",
    "            if name == filename:\n",
    "                return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for extracting the corresponding normalised input and the ground truth output of a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_Xy(file_path):\n",
    "    \n",
    "    # Obtain dataframe for the csv file (one utterance/instance)\n",
    "    df = pd.read_csv(file_path, sep = \";\")\n",
    "    \n",
    "    # Get filename\n",
    "    filename = df[\"name\"][0][1:-1]  # [1:-1] for removing single quotation marks\n",
    "    \n",
    "    # Clean unnecessary columns\n",
    "    df = df.drop(columns = [\"name\", \"frameTime\"])\n",
    "    \n",
    "    # Normalise data\n",
    "    x = df.values                # dataframe to a numpy array\n",
    "    min_max_scalar = MinMaxScaler()  \n",
    "    x_scaled = min_max_scalar.fit_transform(x)     # scaling each feature (each column) to range: (0,1)\n",
    "    df = pd.DataFrame(x_scaled)  # numpy array back to dataframe\n",
    "    \n",
    "    # Obtain matrix X and label y\n",
    "    x_data = sequence.pad_sequences(df.values.T, padding='post', dtype='float64', maxlen=MAX_LEN).T\n",
    "    \n",
    "    y_data = get_label(filename)\n",
    "    \n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.09267241, 0.96610406, 0.54997998, ..., 0.31054128, 0.40723506,\n",
       "         0.48860733],\n",
       "        [0.08859053, 0.98569684, 0.55554785, ..., 0.31111108, 0.35876168,\n",
       "         0.48860733],\n",
       "        [0.08139378, 1.        , 0.52159077, ..., 0.30940166, 0.29020727,\n",
       "         0.48860733],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]]), 'ang')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_Xy(\"/work/peiyun/data/output/dev/IS09_emotion/Ses05M_script03_2_M040_IS09_emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS09 has 32 LLDs and IS10 has 76 LLDs\n",
    "def get_feature_num(config):\n",
    "    \n",
    "    # enter the folder for the config under train (can also use test/dev, same result)\n",
    "    path = os.path.join(data_path, \"output\", \"train\", config)\n",
    "    \n",
    "    # enter any config folder (nums of files are the same for all configs)\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            path = os.path.join(path, filename)\n",
    "            break\n",
    "    \n",
    "    # Obtain dataframe for the csv file (one utterance/instance)\n",
    "    df = pd.read_csv(path, sep = \";\")\n",
    "    \n",
    "    # Clean unnecessary columns\n",
    "    df = df.drop(columns = [\"name\", \"frameTime\"])\n",
    "\n",
    "    return len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the required matrix and label pairs for each data type and store as numpy files. \n",
    "<br>\n",
    "NOTE: np.load(filename.npy) to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_num(data_type):\n",
    "    \n",
    "    # enter the folder for the data_type\n",
    "    path = os.path.join(data_path, \"output\", data_type)\n",
    "    \n",
    "    # enter any config folder (nums of files are the same for all configs)\n",
    "    for config in os.listdir(path):\n",
    "        path = os.path.join(path, config)\n",
    "        break\n",
    "        \n",
    "    return len(os.listdir(path))/2   # only half are csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = os.path.join(data_path, \"temp\")\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_dir = \"numpy_var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in [\"IS09_emotion\", \"IS10_paraling\"]:\n",
    "    \n",
    "    for data_type in [\"train\", \"test\", \"dev\"]:\n",
    "\n",
    "        # temp dat file for storing X\n",
    "        X_data_filename = os.path.join(temp_dir, data_type + \".dat\")\n",
    "        \n",
    "        # Obtain the number of features and number of files\n",
    "        feature_num = get_feature_num(config)\n",
    "        file_num = get_file_num(data_type)\n",
    "        \n",
    " \n",
    "        # initialise lists (using memory-map for accessing small segments of large files on disk)\n",
    "        X = np.memmap(X_data_filename, dtype='float64', mode='w+', shape=((file_num, MAX_LEN, feature_num)))\n",
    "        Y = []\n",
    "\n",
    "        # path for the directory of each data_type\n",
    "        path = os.path.join(data_path, \"output\", data_type, config)\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        # iterate through all files\n",
    "        for filename in os.listdir(path):\n",
    "\n",
    "            # only interested in csv files\n",
    "            if not filename.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            # extract data\n",
    "            x_data, y_data = extract_Xy(os.path.join(path, filename))\n",
    "            X[i] = x_data\n",
    "            Y.append(y_data)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print \"num of file processed: \" + str(i)\n",
    "\n",
    "        # save as numpy file\n",
    "        np.save(os.path.join(data_path, numpy_dir, data_type + \"_\" + config + \"_X.npy\"), X)\n",
    "        np.save(os.path.join(data_path, numpy_dir, data_type + \"_\" + config + \"_Y.npy\"), Y)\n",
    "        \n",
    "        # delete temp data\n",
    "        del X\n",
    "        \n",
    "# delete temp dir\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Read all the required data from the numpy files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for obtaining the required data for the configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables(config):\n",
    "    \n",
    "    # variables\n",
    "    X_train = np.load(os.path.join(data_path, numpy_dir, \"train_\" + config + \"_X.npy\"), mmap_mode='r')\n",
    "    y_train = np.load(os.path.join(data_path, numpy_dir, \"train_\" + config + \"_Y.npy\"), mmap_mode='r')\n",
    "    \n",
    "    X_test = np.load(os.path.join(data_path, numpy_dir, \"test_\" + config + \"_X.npy\"), mmap_mode='r')\n",
    "    y_test = np.load(os.path.join(data_path, numpy_dir, \"test_\" + config + \"_Y.npy\"), mmap_mode='r')\n",
    "    \n",
    "    X_dev = np.load(os.path.join(data_path, numpy_dir, \"dev_\" + config + \"_X.npy\"), mmap_mode='r')\n",
    "    y_dev = np.load(os.path.join(data_path, numpy_dir, \"dev_\" + config + \"_Y.npy\"), mmap_mode='r')\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_dev, y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading all data for config: IS09_emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, X_dev, y_dev = get_variables(\"IS09_emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Define pre-activation residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(input_layer):   # containing: convolution + gated linear unit\n",
    "    \n",
    "    # Obtain convolution layer (1D, temporal convolution) by creating a convolution kernel\n",
    "    tanh_out = Conv1D(NUM_FILTER, \n",
    "                      kernel_size = 2,  # length of 1D convolution window\n",
    "                      kernel_initializer= \"random_uniform\",  # initialization of filters\n",
    "                      dilation_rate=dilation_rate\n",
    "                      padding= \"same\")(input_layer)   # Conv1D “depending” on the input layer\n",
    "\n",
    "    # Normalise the result after Conv1D\n",
    "    tanh_out = BatchNormalization()(tanh_out)\n",
    "\n",
    "    # Obtain the convolutional layer with sigmoid transformation (activation)\n",
    "    sigmoid_out = Conv1D(NUM_FILTER, \n",
    "                         kernel_size = 2,\n",
    "                         kernel_initializer='random_uniform',\n",
    "                         dilation_rate= DILATION_RATE,\n",
    "                         padding= \"same\")(input_layer)\n",
    "    \n",
    "    # Normalise the layer\n",
    "    sigmoid_out = BatchNormalization()(sigmoid_out)\n",
    "    \n",
    "    # Activation function for the layer\n",
    "    sigmoid_out = Activation(\"sigmoid\")(sigmoid_out)\n",
    "\n",
    "    # Element-wise multiplication\n",
    "    merged = multiply([tanh_out, sigmoid_out])\n",
    "    \n",
    "    # Max Pooling for the merged result\n",
    "    merged = MaxPooling1D(pool_size = 2)(merged)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Define a function for generating the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(input_shape):  # input shape: num_files x time x LLD *****\n",
    "    \n",
    "    input_layer = Input(input_shape[1], input_shape[2])  # time x LLD\n",
    "    gated_cnn = residual_block(input_layer)  # first block\n",
    "    \n",
    "    for i in range(0, N_STACK - 1):  # the rest blocks\n",
    "        gated_cnn = residual_block(gated_cnn)\n",
    "    \n",
    "    # Flattening\n",
    "    gated_cnn = Flatten()(gated_cnn)\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    gated_cnn = Dense(256, kernel_initializer='random_uniform')(gated_cnn) \n",
    "    gated_cnn = BatchNormalization()(gated_cnn)\n",
    "    gated_cnn = Activation('relu')(gated_cnn)\n",
    "    gated_cnn = Dropout(0.5)(gated_cnn)\n",
    "    \n",
    "    # Fully Connected Layer then sigmoid\n",
    "    gated_cnn = Dense(1, activation='sigmoid')(gated_cnn)\n",
    "\n",
    "    all_model = Model(inputs = input_layer, outputs = gated_cnn) # This model will include all layers required in the \n",
    "                                                                 # computation of \"outputs\" given \"inputs\"\n",
    "    \n",
    "    # Compiling model\n",
    "#     sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)  # optimizer\n",
    "#     all_model.compile(loss='binary_crossentropy',\n",
    "#                       optimizer='sgd',\n",
    "#                       metrics=[\"accuracy\"])\n",
    "\n",
    "    all_model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy', precision, recall, fscore])\n",
    "#     all_model.summary()\n",
    "    \n",
    "    return all_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Training a GCNN model with the feature configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for training a GCN model with the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X_train, y_train, X_test, y_test, X_dev, y_dev, model_name):\n",
    "    model = generate_model()  # model after compilation\n",
    "    \n",
    "    # fitting training data\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = NUM_EPOCH\n",
    "              validation_data = (X_dev, y_dev),\n",
    "              shuffle = True)\n",
    "    \n",
    "    # saving model\n",
    "    model.save(model_name + \".model\")  # save model\n",
    "    \n",
    "#     # load model\n",
    "#     new_model = load_model(\"epic_num_reader.model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin training model by calling the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(X_train, y_train, X_test, y_test, X_dev, y_dev, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
